name: Sync Conformance Tests

on:
  # Run the workflow every week
  schedule:
    - cron: '0 0 * * 0' # Every Sunday at midnight UTC

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Environment variables to make configuration easy
env:
  # The repository you want to sync FROM (e.g., 'owner/another-repo')
  REMOTE_REPO: 'IntersectMBO/plutus'
  # The specific directory within that repository you want to sync
  REMOTE_DIR: 'plutus-conformance/test-cases'
  # The directory in THIS repository where you want to place the synced content
  LOCAL_DIR: 'plutus/tests/conformance'
  # The branch of the remote repository to check
  REMOTE_BRANCH: 'master'

jobs:
  sync:
    name: Sync Conformance Test Cases
    runs-on: ubuntu-latest

    # Permissions needed to check out the repo and push changes
    permissions:
      contents: write

    steps:
      - name: 1. Checkout current repository
        uses: actions/checkout@v4

      - name: 2. Get latest commit hash from remote directory
        id: get_remote_hash
        run: |
          # Use the GitHub CLI to query the API for the latest commit affecting the target directory
          REMOTE_HASH=$(gh api repos/${{ env.REMOTE_REPO }}/commits --jq '.[0].sha' --method GET -f path=${{ env.REMOTE_DIR }} -f sha=${{ env.REMOTE_BRANCH }})

          if [ -z "$REMOTE_HASH" ]; then
            echo "Could not fetch remote hash. The path or repository might be incorrect."
            exit 1
          fi

          echo "Latest remote hash for '${{ env.REMOTE_DIR }}' is $REMOTE_HASH"
          echo "remote_hash=$REMOTE_HASH" >> $GITHUB_OUTPUT
        env:
          # The GITHUB_TOKEN is automatically provided by GitHub Actions
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: 3. Get last synced commit hash
        id: get_local_hash
        run: |
          # Check if the hash file exists. If not, set hash to an empty string to force an update.
          if [ -f "${{ env.LOCAL_DIR }}/.last_synced_hash" ]; then
            LOCAL_HASH=$(cat "${{ env.LOCAL_DIR }}/.last_synced_hash")
            echo "Last synced hash is $LOCAL_HASH"
          else
            LOCAL_HASH=""
            echo "No local hash file found. Will perform initial sync."
          fi
          echo "local_hash=$LOCAL_HASH" >> $GITHUB_OUTPUT

      - name: 4. Compare hashes and decide to sync
        id: check_hashes
        if: steps.get_remote_hash.outputs.remote_hash != steps.get_local_hash.outputs.local_hash
        run: |
          echo "Hashes do not match. Proceeding with sync."
          echo "needs_sync=true" >> $GITHUB_OUTPUT

      - name: 5. Sync the directory if needed
        if: steps.check_hashes.outputs.needs_sync == 'true'
        run: |
          echo "Starting sync from ${{ env.REMOTE_REPO }}/${{ env.REMOTE_DIR }}"

          # Create a temporary directory for the sparse checkout
          TEMP_CLONE_DIR="temp_clone"
          mkdir -p $TEMP_CLONE_DIR

          # Perform sparse checkout
          cd $TEMP_CLONE_DIR
          git init
          git remote add origin https://github.com/${{ env.REMOTE_REPO }}.git
          git config core.sparseCheckout true

          # Tell git which directory we want
          echo "${{ env.REMOTE_DIR }}/*" > .git/info/sparse-checkout

          # Pull only the required files
          git pull --depth=1 origin ${{ env.REMOTE_BRANCH }}
          cd ..

          # Create the local directory if it doesn't exist
          mkdir -p "${{ env.LOCAL_DIR }}"

          # Use rsync to copy the contents, deleting files that no longer exist in the source
          rsync -av --delete --exclude=".git" "$TEMP_CLONE_DIR/${{ env.REMOTE_DIR }}/" "${{ env.LOCAL_DIR }}/"

          # Store the new hash in the synced directory for the next check
          echo "${{ steps.get_remote_hash.outputs.remote_hash }}" > "${{ env.LOCAL_DIR }}/.last_synced_hash"

          # Clean up the temporary clone
          rm -rf $TEMP_CLONE_DIR
          echo "Sync complete."

      - name: 6. Remove comments (respecting multi-line strings) from .uplc files
        if: steps.check_hashes.outputs.needs_sync == 'true'
        run: |
          echo "Removing comments from .uplc files, respecting multi-line double-quoted strings..."


          find "${{ env.LOCAL_DIR }}" -type f -name "*.uplc" -exec python3 -c '
          import sys
          with open(sys.argv[1]) as f:
              content = f.read()
          result, in_string, i = [], False, 0
          while i < len(content):
              if content[i:i+2] == "\\\\":
                  result.append(content[i:i+2]); i += 2
              elif content[i:i+2] == "\\\"":
                  result.append(content[i:i+2]); i += 2
              elif content[i] == "\"":
                  in_string = not in_string; result.append(content[i]); i += 1
              elif not in_string and content[i:i+2] == "--":
                  i = content.find("\n", i)
                  if i == -1: break
                  result.append("\n"); i += 1
              else:
                  result.append(content[i]); i += 1
          with open(sys.argv[1], "w") as f:
              f.write("".join(result))
          ' {} \;

          echo "Comment removal complete."

          
      - name: 7. Get latest uplc release tag
        id: get-release
        shell: bash
        run: |
          # Fetch the latest release data and extract the tag name
          LATEST_TAG=$(curl -sL https://api.github.com/repos/IntersectMBO/plutus/releases/latest | jq -r .tag_name)
          echo "tag=$LATEST_TAG" >> $GITHUB_OUTPUT

      - name: 8. Cache uplc binary
        id: cache-uplc
        uses: actions/cache@v4
        with:
          # Cache the binary in the runner's tool cache
          path: ${{ runner.tool_cache }}/uplc/uplc
          # The key invalidates when the latest release tag changes
          key: ${{ runner.os }}-uplc-${{ steps.get-release.outputs.tag }}
          restore-keys: |
            ${{ runner.os }}-uplc-

      - name: 9. Download uplc binary (if not cached)
        if: steps.cache-uplc.outputs.cache-hit != 'true'
        shell: bash
        run: |
          echo "Cache miss. Downloading new uplc binary..."
          
          # Find the download URL for the asset that *starts with* 'uplc-x86_64-linux'
          # This handles variations like 'uplc-x86_64-linux-ghc96' or 'uplc-x86_64-linux'
          DOWNLOAD_URL=$(curl -sL https://api.github.com/repos/IntersectMBO/plutus/releases/latest | \
                         jq -r '.assets[] | select(.name | test("^uplc-x86_64-linux")) | .browser_download_url' | \
                         head -n 1) # Take the first match

          if [ -z "$DOWNLOAD_URL" ]; then
            echo "Error: Could not find a matching uplc binary in the latest release."
            exit 1
          fi
          
          echo "Downloading from $DOWNLOAD_URL"
          mkdir -p ${{ runner.tool_cache }}/uplc
          curl -L "$DOWNLOAD_URL" -o ${{ runner.tool_cache }}/uplc/uplc
          chmod +x ${{ runner.tool_cache }}/uplc/uplc

      - name: 10. Add uplc to PATH
        shell: bash
        run: |
          echo "${{ runner.tool_cache }}/uplc" >> $GITHUB_PATH

      - name: 11. Verify uplc installation
        shell: bash
        run: |
          echo "uplc binary found at: $(which uplc)"
          uplc --version

      - name: 12. Generate flat UPLC files
        shell: bash
        run: |
          TARGET_DIR="plutus/tests/conformance"
          
          if [ ! -d "$TARGET_DIR" ]; then
            echo "Error: Directory $TARGET_DIR does not exist."
            exit 1
          fi

          echo "Starting processing in $TARGET_DIR..."
          
          # Find all directories within the target directory
          find "$TARGET_DIR" -type d | while read -r dir; do
            
            # Check if this directory is a leaf (contains no subdirectories)
            if [ -z "$(find "$dir" -mindepth 1 -maxdepth 1 -type d)" ]; then
              
              dirname=$(basename "$dir")
              echo "--- Processing leaf directory: $dirname ---"

              # Define file paths based on the directory name
              input_uplc="$dir/$dirname.uplc"
              input_expected="$dir/$dirname.uplc.expected"
              output_flat="$dir/$dirname.flat"
              output_expected="$dir/$dirname.flat.expected"

              # Check if the required input files exist
              if [ -f "$input_uplc" ] && [ -f "$input_expected" ]; then
                echo "Found $input_uplc"
                echo "Found $input_expected"
                
                # Run the conversion for the main file
                echo "Running: uplc convert -i \"$input_uplc\" -o \"$output_flat\" --of flat"
                uplc convert -i "$input_uplc" -o "$output_flat" --of flat
                
                # Run the conversion for the expected file
                echo "Running: uplc convert -i \"$input_expected\" -o \"$output_expected\" --of flat"
                uplc convert -i "$input_expected" -o "$output_expected" --of flat
                
                echo "Conversion complete for $dirname."
              else
                echo "Warning: Skipping $dir. Could not find expected files:"
                [ ! -f "$input_uplc" ] && echo "  Missing: $input_uplc"
                [ ! -f "$input_expected" ] && echo "  Missing: $input_expected"
              fi
            fi
          done
          
          echo "--- Processing finished ---"

      - name: 13. Commit and push changes
        if: steps.check_hashes.outputs.needs_sync == 'true'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: Sync updates for ${{ env.LOCAL_DIR }} from ${{ env.REMOTE_REPO }}"
          file_pattern: "${{ env.LOCAL_DIR }}/**" # Commit only changes within the target directory
          commit_user_name: "GitHub Actions Bot"
          commit_user_email: "github-actions[bot]@users.noreply.github.com"
          commit_author: "GitHub Actions Bot <github-actions[bot]@users.noreply.github.com>"
